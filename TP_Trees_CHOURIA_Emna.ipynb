{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8OIJnHSo1_T"
      },
      "source": [
        "# Alzheimer prediction using gray matter density from T1w MRI "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN4bwTpJo1_Z"
      },
      "source": [
        "**Deadline**: Upload this notebook (rename it as 'TP4-Trees-YOUR-SURNAME.ipynb') with your answers and code to the Moodle/Ecampus before the deadline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB1qko7fo1_g"
      },
      "source": [
        "We will use a dataset composed of neuroimaging features from brain T1w MR images of 752 subjects, 416 controls and 336 with Alzheimer’s disease. Following the pipeline described in [1], all images are first normalized to a\n",
        "common space, providing a voxel-wise correspondence across subjects. Then, gray matter density is computed at each voxel and averaged over a set of ROIs (Region of Interest) of an atlas, at the beginning you will use the [AAL2 atlas](http://www.gin.cnrs.fr/en/tools/aal/). Data comes from several freely available datasets, like [ADNI](http://adni.loni.usc.edu/) and [OASIS](https://www.oasis-brains.org/), and has been pre-processed by the [Clinica](http://www.clinica.run/) team using the procedure explained in [1].\n",
        "\n",
        "Please load the data from the file: *dataTP.npz* where *T1xxxx* is a matrix containing the averaged density (each row is a subject and each column a feature), *y* is a vector containing the diagnosis (0 for controls and 1 for Alzheimer’s patients) and *ROIlabelsx* contains the name of the ROI of each feature. Here, *x* can take the name of the three atlases you have at your disposal: AAL2, [AICHA](http://www.gin.cnrs.fr/fr/outils/aicha/), [HAMMERS](https://brain-development.org/brain-atlases/adult-brain-atlases/).\n",
        "\n",
        "**Reference**:\n",
        "[1] J. Samper-González, N. Burgos, S. Bottani, S. Fontanella, P. Lu, A. Marcoux, A. Routier, J. Guillon, M. Bacci, J. Wen, A. Bertrand, H. Bertin, M.-O. Habert, S. Durrleman, T. Evgeniou, O. Colliot. *Reproducible evaluation of classification methods in Alzheimer's disease: framework and application to MRI and PET data*. NeuroImage, 2018 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
      ],
      "metadata": {
        "id": "0zBrNMRtTiYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdBPjnmyo1_k"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=ImportWarning)\n",
        "\n",
        "# Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install -q nilearn\n",
        "from nilearn import plotting\n",
        "%matplotlib inline\n",
        "np.random.seed(seed=666)\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TK-_zzzFUyE"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='11cQmPm64k3T7ml5fPLetZgb1j1AjHBH8',\n",
        "dest_path='./dataTP.npz')\n",
        "gdd.download_file_from_google_drive(file_id='1S7e5IrPygE4VV0JTwqJIlyO2S_NhsiI4',\n",
        "dest_path='./AtlasAAL2.nii')\n",
        "gdd.download_file_from_google_drive(file_id='1E0pu5jIMpgcs2DQ8lBGWliwEBZvKrnV9',\n",
        "dest_path='./AtlasAICHA.nii')\n",
        "gdd.download_file_from_google_drive(file_id='1yltKwULrkHYh79RAh_zAg08r8pQMjRlQ',\n",
        "dest_path='./AtlasHAMMERS.nii')\n",
        "\n",
        "with np.load('./dataTP.npz',allow_pickle=True) as data:\n",
        "    T1AAL2 = data['T1AAL2'] # data from AAL2 Atlas\n",
        "    T1AICHA = data['T1AICHA'] # data from AICHA Atlas\n",
        "    T1HAMMERS = data['T1HAMMERS'] # data from HAMMERS Atlas  \n",
        "    y = data['y'] # classes, 0 for controls and 1 for patients    \n",
        "    ROIlabelsAAL2 = data['ROIlabelsAAL2'] # labels for ROIs of atlas AAL2 \n",
        "    ROIlabelsAICHA = data['ROIlabelsAICHA']    # labels for ROIs of atlas AICHA \n",
        "    ROIlabelsHAMMERS = data['ROIlabelsHAMMERS']    # labels for ROIs of atlas HAMMERS "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-kuwBwFIilJ"
      },
      "source": [
        "# Choose Atlas (here AAL2)\n",
        "X=T1AAL2 # T1AAL2, T1AICHA, T1HAMMERS\n",
        "labels=ROIlabelsAAL2 # ROIlabelsAAL2, ROIlabelsAICHA, ROIlabelsHAMMERS\n",
        "atlas='./AtlasAAL2.nii' #AtlasAAL2.nii, AtlasAICHA.nii, AtlasHAMMERS.nii\n",
        "\n",
        "N,M = X.shape # number subjects and ROIs\n",
        "class_names = [\"control\",\"alzheimer\"] # y=0, y=1\n",
        "\n",
        "print('Number of controls and Alzheimer patients is respectively: {0} and {1}'.format(N-np.sum(y), np.sum(y)))\n",
        "print('Number of ROI (features) is: {0}'.format(M))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pk4GnWxo1_7"
      },
      "source": [
        "Using the library nilearn we can also plot the atlas used to define the ROIs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LUtw8L2o1_-"
      },
      "source": [
        "plotting.plot_roi(atlas, title=atlas)\n",
        "plotting.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FIMJctdo2AE"
      },
      "source": [
        "In this TP we will use Decision Trees, Bagging and Random Forests. Let's start with Decision Trees. First of all, we need to create a training and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LgMdc3xo2AK"
      },
      "source": [
        "# Create training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE9pu31U4srj"
      },
      "source": [
        "And then we need to check whether out features need to be standardized or normalized. Let's have a look at them. Remember that if features can have both negative and positive values, as a rule of thumb, they should be standardized. If they only have positive values, a normalization is usually used. \n",
        "\n",
        "As already said, please remember that you should learn the standardization/normalization (namely learn the average/std or the max/min values) ONLY in the training set and then use the same values also in the test set. You should NOT use the entire dataset (both training and test) for standardization/normalization. Otherwise, you would have a *data leakage*, namely you would use data (the test set) that you should not use during training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI7Z_wPG5Del"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.boxplot(X,notch=True);\n",
        "\n",
        "# Standardization/Normalization\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scale=scaler.transform(X_train)\n",
        "X_test_scale=scaler.transform(X_test)\n",
        "\n",
        "normalizer = MinMaxScaler()\n",
        "normalizer.fit(X_train)\n",
        "X_train_normalize=normalizer.transform(X_train)\n",
        "X_test_normalize=normalizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1HN5Uy2o2AO"
      },
      "source": [
        "Then, we can fit a Decision tree, with the default setting, using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTe1dj0No2AP"
      },
      "source": [
        "# Fitting Decision Trees \n",
        "Tree = DecisionTreeClassifier(random_state=0)\n",
        "Tree.fit(X_train_scale,y_train)\n",
        "# Score in the training set\n",
        "print('Score in the training set is {0}'.format(Tree.score(X_train_scale,y_train)) )\n",
        "# Score in the test set\n",
        "print('Score in the test set is {0}'.format(Tree.score(X_test_scale,y_test)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5HzkZCPo2Ab"
      },
      "source": [
        "Instead than using the default hyperparameters, we could also look for the best ones. Among the hyperparameters implemented in *scikit-learn* we could use *'min_samples_split'*, the minimum number of samples required to split an internal node, and/or *'min_samples_leaf'*, the minimum number of samples required to be present at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. \n",
        "\n",
        "Plot the training and test score for different values of 'min_samples_split' (for instance between 2 and 15) WITHOUT using Cross Validation. Do the same for 'min_samples_leaf'.\n",
        "\n",
        "**Question:** What is the best value ? What happens if you split differently your data (change `random_state`in the function `train_test_split`) ? \n",
        "\n",
        "**Answer:** The best value minimizes the error for the test set  and it is at least 3 samples required to split an internal node.\n",
        "If we split differently the data by changing the random_state in the function train_test_split, the best value changes for the test set. \n",
        "However the performance changes only very slightly when we change these different parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKVCOi5oo2Ac"
      },
      "source": [
        "# min_samples_split\n",
        "\n",
        "TTest=[]\n",
        "TTrain=[]\n",
        "for i in range(2,16):\n",
        "    Tree = DecisionTreeClassifier(min_samples_split=i, random_state=0)\n",
        "    Tree.fit(X_train_scale,y_train)\n",
        "    scoreTrain=Tree.score(X_train_scale,y_train)\n",
        "    scoreTest=Tree.score(X_test_scale,y_test)\n",
        "    TTrain.append(scoreTrain)\n",
        "    TTest.append(scoreTest)\n",
        "plt.plot(TTrain,label='Training error');\n",
        "plt.xticks(np.arange(14), ('2', '3', '4', '5', '6','7','8','9','10','11','12','13','14','15'))\n",
        "print(\"The value of min_samples_split that maximizes the training score is : \",TTrain.index(max(TTrain))+2)\n",
        "plt.plot(TTest,label='Test error');\n",
        "plt.xticks(np.arange(14), ('2', '3', '4', '5', '6','7','8','9','10','11','12','13','14','15'))\n",
        "plt.xlabel('min samples split')\n",
        "plt.ylabel('score')\n",
        "print(\"The value of min_samples_split that maximizes the test score is : \",TTest.index(max(TTest))+2)\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3d7pHgoo2Aj"
      },
      "source": [
        "# min_samples_split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.3, random_state=10,stratify=y)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scale2=scaler.transform(X_train2)\n",
        "X_test_scale2=scaler.transform(X_test2)\n",
        "\n",
        "TTest=[]\n",
        "TTrain=[]\n",
        "for i in range(2,16):\n",
        "    Tree = DecisionTreeClassifier(min_samples_split=i, random_state=0)\n",
        "    Tree.fit(X_train_scale2,y_train2)\n",
        "    scoreTrain=Tree.score(X_train_scale2,y_train2)\n",
        "    scoreTest=Tree.score(X_test_scale2,y_test2)\n",
        "    TTrain.append(scoreTrain)\n",
        "    TTest.append(scoreTest)\n",
        "plt.plot(TTrain,label='Training error');\n",
        "plt.xticks(np.arange(14), ('2', '3', '4', '5', '6','7','8','9','10','11','12','13','14','15'))\n",
        "print(\"The value of min_samples_split that maximizes the training score is : \",TTrain.index(max(TTrain))+2)\n",
        "plt.plot(TTest,label='Test error');\n",
        "plt.xticks(np.arange(14), ('2', '3', '4', '5', '6','7','8','9','10','11','12','13','14','15'))\n",
        "plt.xlabel('min samples split')\n",
        "plt.ylabel('score')\n",
        "print(\"The value of min_samples_split that maximizes the test score is : \",TTest.index(max(TTest))+2)\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# min_samples_leaf\n",
        "\n",
        "TTest=[]\n",
        "TTrain=[]\n",
        "for i in range(2,16):\n",
        "    Tree = DecisionTreeClassifier(min_samples_leaf=i, random_state=0)\n",
        "    Tree.fit(X_train_scale,y_train)\n",
        "    scoreTrain=Tree.score(X_train_scale,y_train)\n",
        "    scoreTest=Tree.score(X_test_scale,y_test)\n",
        "    TTrain.append(scoreTrain)\n",
        "    TTest.append(scoreTest)\n",
        "plt.plot(TTrain,label='Training error');\n",
        "plt.xticks(np.arange(14), ('2', '3', '4', '5', '6','7','8','9','10','11','12','13','14','15'))\n",
        "print(\"The value of min_samples_leaf that maximizes the training score is : \",TTrain.index(max(TTrain))+2)\n",
        "plt.plot(TTest,label='Test error');\n",
        "plt.xticks(np.arange(14), ('2', '3', '4', '5', '6','7','8','9','10','11','12','13','14','15'))\n",
        "print(\"The value of min_samples_leaf that maximizes the test score is : \",TTest.index(max(TTest))+2)\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "ZIzujcNgBz5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI1gFa97o2Ao"
      },
      "source": [
        "Try to add Gaussian noise to the data (using for instance zero mean and 0.05 for $\\sigma$) and, using the best hyperparameters found before in the test set (you can use both `min_samples_leaf` and `min_samples_split`), look at the test score. Repeat this process several times and compare the results with the score obtained without adding noise. \n",
        "\n",
        "**Question**: Are the results stable ? Hint: you could use for instance *noise = np.random.normal(mu, sigma)* if you have standardized the features and *noise = np.abs(np.random.normal(mu, sigma))* if you have normalized them (we use *np.asb()* to take only positive values and $\\sigma$ should be small in order to (almost) preserve the range of the features between 0 and 1)\n",
        "\n",
        "**Answer**: No the results are not stable. Indeed descion trees suffer from variance which refers to the sensitivity of a model's predictions to small changes in the training data. Thus a decision tree with high variance may produce very different trees when trained on slightly different subsets of the data, and may perform poorly on new data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rppiIbQFo2Ap"
      },
      "source": [
        "#adding noise to the data\n",
        "Tree = DecisionTreeClassifier(min_samples_split=3,min_samples_leaf=10,random_state=0)\n",
        "scoreTestnoise=np.zeros(100)\n",
        "scoreTest=np.zeros(100)\n",
        "\n",
        "Tree.fit(X_train_scale,y_train)\n",
        "scoreTest[:] = Tree.score(X_test_scale,y_test)  \n",
        "\n",
        "\n",
        "for k in range(100):   \n",
        "    #X_train_scale_noise=X_train_scale+np.random.normal(0,0.05,X_train_scale.shape)\n",
        "    X_train_temp=np.copy(X_train_scale)\n",
        "    for i in range(X_train.shape[0]):\n",
        "        X_train_temp[i] = X_train_scale[i]+np.abs(np.random.normal(0,0.05))\n",
        "        \n",
        "    Tree.fit(X_train_temp,y_train)\n",
        "    scoreTestnoise[k] = Tree.score(X_test_scale,y_test)\n",
        "    \n",
        "plt.plot(scoreTestnoise,'b',label='noise')\n",
        "plt.plot(scoreTest,'r',linewidth=5.0,label='original')       \n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrLs1nn8o2AU"
      },
      "source": [
        "To plot decision trees, we can also use the *graphviz* library. If you need to install it locally, you can do it using *conda install python-graphviz*. \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkOG-YOvks1E"
      },
      "source": [
        "First plot the tree learnt on the original data, witout adding noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2fXiMYIo2AW"
      },
      "source": [
        "import graphviz \n",
        "\n",
        "Tree.fit(X_train_scale,y_train)\n",
        "dot_data = tree.export_graphviz(Tree, out_file=None,feature_names=labels,class_names=class_names,filled=True, rounded=True,special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPDvOh79mcrj"
      },
      "source": [
        "Now, plot the tree learnt on noisy data.\n",
        "\n",
        "**Question**: Is it the same ? You can try several times, for different levels of noise. Comment the results\n",
        "\n",
        "**Answer**:\n",
        "The learnt tree on noisy data is not the same depending on the level of noise.\n",
        "Noisy features contain random or irrelevant information that can make it harder for a model to accurately learn the underlying pattern in the data. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_temp=np.copy(X_train_scale)\n",
        "for i in range(X_train.shape[0]):\n",
        "    X_train_temp[i] = X_train_scale[i]+np.abs(np.random.normal(0,0.05))\n",
        "Tree.fit(X_train_temp,y_train)\n",
        "dot_data = tree.export_graphviz(Tree, out_file=None,feature_names=labels,class_names=class_names,filled=True, rounded=True,special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "id": "wMgpc9YdNlIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_temp=np.copy(X_train_scale)\n",
        "for i in range(X_train.shape[0]):\n",
        "    X_train_temp[i] = X_train_scale[i]+np.abs(np.random.normal(0,0.1))\n",
        "Tree.fit(X_train_temp,y_train)\n",
        "dot_data = tree.export_graphviz(Tree, out_file=None,feature_names=labels,class_names=class_names,filled=True, rounded=True,special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "id": "L_f5zmsKNpD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_temp=np.copy(X_train_scale)\n",
        "for i in range(X_train.shape[0]):\n",
        "    X_train_temp[i] = X_train_scale[i]+np.abs(np.random.normal(0,0.5))\n",
        "Tree.fit(X_train_temp,y_train)\n",
        "dot_data = tree.export_graphviz(Tree, out_file=None,feature_names=labels,class_names=class_names,filled=True, rounded=True,special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "id": "8yY4wcvSNxE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_temp=np.copy(X_train_scale)\n",
        "for i in range(X_train.shape[0]):\n",
        "    X_train_temp[i] = X_train_scale[i]+np.abs(np.random.normal(0,1))\n",
        "Tree.fit(X_train_temp,y_train)\n",
        "dot_data = tree.export_graphviz(Tree, out_file=None,feature_names=labels,class_names=class_names,filled=True, rounded=True,special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "id": "HF9TJeJONyn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLQwNNCco2At"
      },
      "source": [
        "Instead than using a single split of the data, we could also use Cross Validation to compute the best hyperparameter values for both 'min_samples_split' and 'min_samples_leaf' at the same time and in an automatic way. \n",
        "\n",
        "**Question:** Do you find the same optimal hyperparameters as before ? Hint: use GridSearchCV\n",
        "\n",
        "**Answer:** No, the optimal hyperparameters are not the same as before because the random split in cross validation gives different results.\n",
        "\n",
        "**Question**: So far, we have used the standard score (ie accuracy). Would you use a different one ? If yes, which one and why ?\n",
        "\n",
        "**Answer**: We can use other metrics such as recall which gives worse results or precision which gives slightly better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDvf_RYTo2Av"
      },
      "source": [
        "Tree = DecisionTreeClassifier()\n",
        "p_grid_tree = {'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15],'min_samples_leaf': [2,3,4,5,6,7,8,9,10,11,12,13,14,15] }\n",
        "grid_tree = GridSearchCV(estimator=Tree, param_grid=p_grid_tree, cv=5,scoring=\"accuracy\")\n",
        "grid_tree.fit(X_train_scale,y_train)\n",
        "print(\"Best Validation Score: {}\".format(grid_tree.best_score_))\n",
        "print(\"Best params: {}\".format(grid_tree.best_params_))\n",
        "print(\"Tree test score :\",grid_tree.score(X_test_scale,y_test))\n",
        "best_params=grid_tree.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tree = DecisionTreeClassifier()\n",
        "p_grid_tree = {'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15],'min_samples_leaf': [2,3,4,5,6,7,8,9,10,11,12,13,14,15] }\n",
        "grid_tree = GridSearchCV(estimator=Tree, param_grid=p_grid_tree, cv=5,scoring=\"recall\")\n",
        "grid_tree.fit(X_train_scale,y_train)\n",
        "print(\"Best Validation Score: {}\".format(grid_tree.best_score_))\n",
        "print(\"Best params: {}\".format(grid_tree.best_params_))\n",
        "print(\"Tree test score :\",grid_tree.score(X_test_scale,y_test))\n",
        "best_params=grid_tree.best_params_"
      ],
      "metadata": {
        "id": "Oubdo8s5Sjoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tree = DecisionTreeClassifier()\n",
        "p_grid_tree = {'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15],'min_samples_leaf': [2,3,4,5,6,7,8,9,10,11,12,13,14,15] }\n",
        "grid_tree = GridSearchCV(estimator=Tree, param_grid=p_grid_tree, cv=5,scoring=\"precision\")\n",
        "grid_tree.fit(X_train_scale,y_train)\n",
        "print(\"Best Validation Score: {}\".format(grid_tree.best_score_))\n",
        "print(\"Best params: {}\".format(grid_tree.best_params_))\n",
        "print(\"Tree test score :\",grid_tree.score(X_test_scale,y_test))\n",
        "best_params=grid_tree.best_params_"
      ],
      "metadata": {
        "id": "I8d45wNcS9zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC7SNh5-o2A2"
      },
      "source": [
        "Using the estimated optimal hyperparameers, plot the new decision tree using the *graphviz* library. \n",
        "\n",
        "**Question**: Is it the same as before? Do you see ROIs that are always close to the root of the tree among the different experiments ? If yes, what does it mean in your opinion ? Comment the results.\n",
        "\n",
        "**Answer**: The tree is not the same as before. However we always see the same ROIs close to the root of the tree for the different experiments (adding noise, cross validation). These ROIs are then the most important features for this classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5ucpzowo2A4"
      },
      "source": [
        "Tree = DecisionTreeClassifier(min_samples_leaf=best_params[\"min_samples_leaf\"],min_samples_split=best_params[\"min_samples_split\"], random_state=0)\n",
        "Tree.fit(X_train_scale,y_train)\n",
        "dot_data = tree.export_graphviz(Tree, out_file=None,feature_names=labels,class_names=class_names,filled=True, rounded=True,special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK17gOu9o2A9"
      },
      "source": [
        "Try to use now Bagging. You can use the following code where we use the previously computed best parameters 'min_samples_leaf' and 'min_samples_split'. \n",
        "\n",
        "**Question**: What happens when you use the original data and the noisy version ? Do you notice any difference in the prediction scores with respect to the results using Decision Trees ? \n",
        "\n",
        "**Answer**: The results are slightly worse with noisy data, however the best number of estimators is the same.\n",
        "The prediction scores using  bagging are better than using Decision Trees, almost for noisy data. Bagging can hence help to reduce improve the accuracy and robustness of the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij465Vaho2A-"
      },
      "source": [
        "Tree = DecisionTreeClassifier(min_samples_leaf=best_params[\"min_samples_leaf\"],min_samples_split=best_params[\"min_samples_split\"], random_state=0)\n",
        "p_grid_bagging = {'n_estimators': [5,10,15,20]}      \n",
        "bag=BaggingClassifier(base_estimator=Tree, random_state=0)\n",
        "grid_bagging = GridSearchCV(estimator=bag, param_grid=p_grid_bagging, cv=5,scoring=\"accuracy\")\n",
        "grid_bagging.fit(X_train_scale, y_train)\n",
        "print(\"Best Validation Score: {}\".format(grid_bagging.best_score_))\n",
        "print(\"Best params: {}\".format(grid_bagging.best_params_))\n",
        "print(\"Bagging test score :\",grid_bagging.score(X_test_scale,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91OSfGQwo2BC"
      },
      "source": [
        "# Bagging on noisy data\n",
        "X_train_temp=np.copy(X_train_scale)\n",
        "for i in range(X_train.shape[0]):\n",
        "    X_train_temp[i] = X_train_scale[i]+np.abs(np.random.normal(0,1))\n",
        "p_grid_bagging = {'n_estimators': [5,10,15,20]}      \n",
        "bag=BaggingClassifier(base_estimator=Tree, random_state=0)\n",
        "grid_bagging = GridSearchCV(estimator=bag, param_grid=p_grid_bagging, cv=5,scoring=\"accuracy\")\n",
        "grid_bagging.fit(X_train_temp, y_train)\n",
        "print(\"Best Validation Score: {}\".format(grid_bagging.best_score_))\n",
        "print(\"Best params: {}\".format(grid_bagging.best_params_))\n",
        "print(\"Bagging test score :\",grid_bagging.score(X_test_scale,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STHcxeyfo2BF"
      },
      "source": [
        "The last part of this TP is about Random Forests. We can estimate the three hyperparameters *'n_estimators'*, *'min_samples_leaf'* and *'max_features'*, the number of features to consider when looking for the best split, as before using Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BE-H8ywo2BG"
      },
      "source": [
        "RF=RandomForestClassifier(random_state=0)\n",
        "p_grid_RF = {'n_estimators': [10,15,20,25,30], 'min_samples_leaf': [2,3,4,5,6], 'max_features': ['sqrt','log2']}   \n",
        "\n",
        "grid_RF = GridSearchCV(estimator=RF, param_grid=p_grid_RF, scoring='accuracy', cv=5)\n",
        "grid_RF.fit(X_train_scale, y_train)\n",
        "\n",
        "print(\"Best Validation Score: {}\".format(grid_RF.best_score_))\n",
        "print(\"Best params: {}\".format(grid_RF.best_params_))\n",
        "print(\"Random Forest test score :\",grid_RF.score(X_test_scale,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI414cecG3i3"
      },
      "source": [
        "Using the estimated best hyperparameters, test the performance of Random Forest on the noisy data and compare the results with Decision Trees and Bagging. \n",
        "\n",
        "**Answer**: The results obtained with Random Forest are better than Decision Tress and Bagging almost for the noisy data, so it is an even more robust model than bagging. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnwoUhr3G6b1"
      },
      "source": [
        "# Random Forest on noisy data\n",
        "X_train_temp=np.copy(X_train_scale)\n",
        "for i in range(X_train.shape[0]):\n",
        "    X_train_temp[i] = X_train_scale[i]+np.abs(np.random.normal(0,1))\n",
        "RF=RandomForestClassifier(random_state=0)\n",
        "p_grid_RF = {'n_estimators': [10,15,20,25,30], 'min_samples_leaf': [2,3,4,5,6], 'max_features': ['sqrt','log2']}   \n",
        "\n",
        "grid_RF = GridSearchCV(estimator=RF, param_grid=p_grid_RF, scoring='accuracy', cv=5)\n",
        "grid_RF.fit(X_train_temp, y_train)\n",
        "\n",
        "print(\"Best Validation Score: {}\".format(grid_RF.best_score_))\n",
        "print(\"Best params: {}\".format(grid_RF.best_params_))\n",
        "print(\"Random Forest test score :\",grid_RF.score(X_test_scale,y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbPiUrNAo2BN"
      },
      "source": [
        "We can also use Random Forests to check the importance of the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7PU3C8ko2BQ"
      },
      "source": [
        "best_params=grid_RF.best_params_\n",
        "RF = RandomForestClassifier(min_samples_leaf=best_params[\"min_samples_leaf\"],max_features=best_params[\"max_features\"],n_estimators=best_params[\"n_estimators\"], random_state=0)\n",
        "RF.fit(X_train,y_train)\n",
        "\n",
        "importances = RF.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(10):\n",
        "    print(\"%d. feature %d representing %s (%f)\" % (f + 1, indices[f], labels[indices[f]], importances[indices[f]]))\n",
        "\n",
        "# Plot the feature importances of the forest\n",
        "plt.figure()\n",
        "plt.title(\"Feature importance\")\n",
        "plt.bar(range(10), importances[indices[0:10]], color=\"r\", align=\"center\")\n",
        "plt.xticks(range(10), indices[0:10])\n",
        "plt.xlim([-1, 10])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiNUODxOo2BU"
      },
      "source": [
        "**Question**: Which are the most important features (i.e. ROIs) ?  Based on the two given research papers, you can verify if your results make sense. \n",
        "\n",
        "**Answer**:\n",
        "Based on the two given research papers, the most important features are in the 46 Amygdala_R 4202 and is 45 Amygdala_L 4201. The results we obtain using Random Forest make sense because they show that Amygdala is very important to predict Alzheimer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psuqzjU5o2BY"
      },
      "source": [
        "We can also inspect the data using only pairs of the most important features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om2OxZ3bo2Bb"
      },
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "for pairidx, pair in enumerate([ [indices[0],indices[1]], [indices[0],indices[2]], [indices[0],indices[3]],\n",
        "                                [indices[1],indices[2]], [indices[1],indices[3]], [indices[2],indices[3]] ]):\n",
        "    # We only take the two corresponding features\n",
        "    Xpair = X_train[:, pair]\n",
        "    ypair = y_train\n",
        "\n",
        "    # Train\n",
        "    clf = RF.fit(Xpair, ypair)\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.subplot(2, 3, pairidx + 1)\n",
        "\n",
        "    x_min, x_max = Xpair[:, 0].min() - 1, Xpair[:, 0].max() + 1\n",
        "    y_min, y_max = Xpair[:, 1].min() - 1, Xpair[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                         np.arange(y_min, y_max, 0.02))\n",
        "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "    plt.xlabel(pair[0])\n",
        "    plt.ylabel(pair[1])\n",
        "\n",
        "    # Plot the training points\n",
        "    for i, color in zip(range(2), \"ym\"):\n",
        "        idx = np.where(ypair == i)\n",
        "        plt.scatter(Xpair[idx, 0], Xpair[idx, 1], c=color, label=class_names[i],\n",
        "                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
        "\n",
        "plt.suptitle(\"Decision surface of a random forest using couples of the most important features\")\n",
        "plt.legend(bbox_to_anchor=(1, 0.5))\n",
        "plt.axis(\"tight\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJMWJrFh0mGO"
      },
      "source": [
        "**Question**: Which is the best couple of features ?\n",
        "\n",
        "**Answer**: The best couple of features seems to be 41-45 because it gives more precise separation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZjHYuz7sAIU"
      },
      "source": [
        "**Different Atlas**\n",
        "\n",
        "Previously, we have used the AAL2 which defines a precise split of the brain into ROIs. What happens if you change Atlas ? Do you obtain the same results ? Can you find a subset of ROIs that you could define 'biomarkers' of the Alzheimer's disease ? Justify your answer and check whether it makes sense by using the two given research papers.\n",
        "\n",
        "You can use the AICHA (http://www.gin.cnrs.fr/fr/outils/aicha/) and HAMMERS (https://brain-development.org/brain-atlases/adult-brain-atlases/) atlas.\n",
        "\n",
        "**Answer**:\n",
        "The most important features obtained using HAMMERS are similar to those obtained with AAL2 and are in accordance with the given research papers where Amygdala and Hippocampus are the most important features. However for AICHA we obtain different results assuming that Parietooccipital and Thalamus are the most important ones."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choice of the atlas** : AICHA"
      ],
      "metadata": {
        "id": "zgY4v1U3pIBp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Nxl8vNObJQ"
      },
      "source": [
        "# Choose Atlas (here AAL2)\n",
        "X=T1AICHA # T1AAL2, T1AICHA, T1HAMMERS\n",
        "labels=ROIlabelsAICHA # ROIlabelsAAL2, ROIlabelsAICHA, ROIlabelsHAMMERS\n",
        "atlas='./AtlasAICHA.nii' #AtlasAAL2.nii, AtlasAICHA.nii, AtlasHAMMERS.nii\n",
        "\n",
        "N,M = X.shape # number subjects and ROIs\n",
        "class_names = [\"control\",\"alzheimer\"] # y=0, y=1\n",
        "\n",
        "print('Number of controls and Alzheimer patients is respectively: {0} and {1}'.format(N-np.sum(y), np.sum(y)))\n",
        "print('Number of ROI (features) is: {0}'.format(M))\n",
        "\n",
        "plotting.plot_roi(atlas, title=atlas)\n",
        "plotting.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scale=scaler.transform(X_train)\n",
        "X_test_scale=scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "-jsCwQtDqT8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF=RandomForestClassifier(random_state=0)\n",
        "p_grid_RF = {'n_estimators': [10,15,20,25,30], 'min_samples_leaf': [2,3,4,5,6], 'max_features': ['sqrt','log2']}   \n",
        "\n",
        "grid_RF = GridSearchCV(estimator=RF, param_grid=p_grid_RF, scoring='accuracy', cv=5)\n",
        "grid_RF.fit(X_train_scale, y_train)\n",
        "\n",
        "print(\"Best Validation Score: {}\".format(grid_RF.best_score_))\n",
        "print(\"Best params: {}\".format(grid_RF.best_params_))\n",
        "print(\"Random Forest test score :\",grid_RF.score(X_test_scale,y_test))"
      ],
      "metadata": {
        "id": "i-19uXX1rGfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params=grid_RF.best_params_\n",
        "RF = RandomForestClassifier(min_samples_leaf=best_params[\"min_samples_leaf\"],max_features=best_params[\"max_features\"],n_estimators=best_params[\"n_estimators\"], random_state=0)\n",
        "RF.fit(X_train,y_train)\n",
        "\n",
        "importances = RF.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(10):\n",
        "    print(\"%d. feature %d representing %s (%f)\" % (f + 1, indices[f], labels[indices[f]], importances[indices[f]]))\n",
        "\n",
        "# Plot the feature importances of the forest\n",
        "plt.figure()\n",
        "plt.title(\"Feature importance\")\n",
        "plt.bar(range(10), importances[indices[0:10]], color=\"r\", align=\"center\")\n",
        "plt.xticks(range(10), indices[0:10])\n",
        "plt.xlim([-1, 10])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t4Ls1PSNrSCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choice of the atlas** : HAMMERS"
      ],
      "metadata": {
        "id": "-oIynVg-qCPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose Atlas (here AAL2)\n",
        "X=T1HAMMERS # T1AAL2, T1AICHA, T1HAMMERS\n",
        "labels=ROIlabelsHAMMERS # ROIlabelsAAL2, ROIlabelsAICHA, ROIlabelsHAMMERS\n",
        "atlas='./AtlasHAMMERS.nii' #AtlasAAL2.nii, AtlasAICHA.nii, AtlasHAMMERS.nii\n",
        "\n",
        "N,M = X.shape # number subjects and ROIs\n",
        "class_names = [\"control\",\"alzheimer\"] # y=0, y=1\n",
        "\n",
        "print('Number of controls and Alzheimer patients is respectively: {0} and {1}'.format(N-np.sum(y), np.sum(y)))\n",
        "print('Number of ROI (features) is: {0}'.format(M))\n",
        "\n",
        "plotting.plot_roi(atlas, title=atlas)\n",
        "plotting.show()"
      ],
      "metadata": {
        "id": "8m5u5kpYpgU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scale=scaler.transform(X_train)\n",
        "X_test_scale=scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "knqhdEq_qJeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF=RandomForestClassifier(random_state=0)\n",
        "p_grid_RF = {'n_estimators': [10,15,20,25,30], 'min_samples_leaf': [2,3,4,5,6], 'max_features': ['sqrt','log2']}   \n",
        "\n",
        "grid_RF = GridSearchCV(estimator=RF, param_grid=p_grid_RF, scoring='accuracy', cv=5)\n",
        "grid_RF.fit(X_train_scale, y_train)\n",
        "\n",
        "print(\"Best Validation Score: {}\".format(grid_RF.best_score_))\n",
        "print(\"Best params: {}\".format(grid_RF.best_params_))\n",
        "print(\"Random Forest test score :\",grid_RF.score(X_test_scale,y_test))"
      ],
      "metadata": {
        "id": "HAu9az9IrHw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params=grid_RF.best_params_\n",
        "RF = RandomForestClassifier(min_samples_leaf=best_params[\"min_samples_leaf\"],max_features=best_params[\"max_features\"],n_estimators=best_params[\"n_estimators\"], random_state=0)\n",
        "RF.fit(X_train,y_train)\n",
        "\n",
        "importances = RF.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(10):\n",
        "    print(\"%d. feature %d representing %s (%f)\" % (f + 1, indices[f], labels[indices[f]], importances[indices[f]]))\n",
        "\n",
        "# Plot the feature importances of the forest\n",
        "plt.figure()\n",
        "plt.title(\"Feature importance\")\n",
        "plt.bar(range(10), importances[indices[0:10]], color=\"r\", align=\"center\")\n",
        "plt.xticks(range(10), indices[0:10])\n",
        "plt.xlim([-1, 10])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n-ke51LKrQd5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}